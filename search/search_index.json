{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"prefect-openmetadata","text":"<p>[!WARNING] <code>prefect-openmetadata</code> is no longer actively maintained. For more details, please see the Maintenance Status section below.</p>"},{"location":"#maintenance-status","title":"Maintenance Status","text":"<p><code>prefect-openmetadata</code> has been a valuable part of the Prefect ecosystem. Due to shifts in our strategic priorities, we have decided to discontinue the active maintenance of this library. While we will not be updating the code or addressing issues, the existing codebase will remain accessible for archival purposes. We appreciate the support and contributions from our community.</p>"},{"location":"#welcome","title":"Welcome!","text":"<p>Using Prefect and OpenMetadata together will help you build and maintain a data platform you can trust. </p> <p>Prefect allows you to coordinate your dataflow and provides visibility into the health of your workflow execution and workflow lineage. With OpenMetadata integration, you can enrich your orchestration system with metadata about data lineage, data catalog, data quality and governance, giving you more information about the health of your system. </p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#python-setup","title":"Python setup","text":"<p>Requires an installation of Python 3.8+.</p> <p>We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install <code>prefect-openmetadata</code> with <code>pip</code>:</p> <pre><code>pip install prefect-openmetadata\n</code></pre>"},{"location":"#install-openmetadata-and-prefect","title":"Install <code>OpenMetadata</code> and <code>Prefect</code>","text":"<p>Head over to the install OpenMetadata page for detailed instructions on how to install and configure both platforms.</p>"},{"location":"#write-and-run-metadata-ingestion-flow","title":"Write and run metadata ingestion flow","text":"<pre><code>from prefect_openmetadata.flows import ingest_metadata\n\nconfig = \"\"\"See an example in the section: Run ingestion flow\"\"\"\n\nif __name__ == \"__main__\":\n    ingest_metadata(config)\n</code></pre> <p>For more details, check the run ingestion flow section.</p>"},{"location":"#schedule-a-metadata-ingestion-flow","title":"Schedule a metadata ingestion flow","text":"<p>Simple example: <pre><code>prefect deployment build -a -n dev myflow.py:ingest_metadata --interval 900\n</code></pre></p> <p>For more details, check the schedule ingestion flow section.</p>"},{"location":"#resources","title":"Resources","text":"<p>If you encounter any bugs while using <code>prefect-openmetadata</code>, feel free to open an issue in the prefect-openmetadata repository.</p> <p>If you have any questions or issues while using <code>prefect-openmetadata</code>, you can find help in either the Prefect Discourse forum or the Prefect Slack community.</p>"},{"location":"#development","title":"Development","text":"<p>If you'd like to install a version of <code>prefect-openmetadata</code> for development, clone the repository and perform an editable install with <code>pip</code>:</p> <pre><code>git clone https://github.com/PrefectHQ/prefect-openmetadata.git\n\ncd prefect-openmetadata/\n\npip install -e \".[dev]\"\n\n# Install linting pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"all_workflows/","title":"Run metadata ingestion, usage workflow and data profiler for Snowflake","text":"<p>This demo will walk you through the process of creating ingestion, usage and profiling workflows.</p>"},{"location":"all_workflows/#configure-your-profiling-spec","title":"Configure your profiling spec","text":"<p>OpenMetadata heavily relies on schemas. You need to understand which arguments are required and which are optional, and which are important for your use case. The easiest way to identify the required and optional arguments of any ingestion or profiling workflow is to import the <code>OpenMetadataWorkflowConfig</code>. </p> <p>From this pydantic model, you can drill down into other components using your favorite IDE e.g. Pycharm or VSCode. From there, you can again drill down into <code>Source</code>, <code>SourceConfig</code>, <code>Sink</code>, etc.</p> <pre><code>from metadata.generated.schema.metadataIngestion.workflow import (\n    OpenMetadataWorkflowConfig, Source, SourceConfig, Sink\n)\n</code></pre>"},{"location":"all_workflows/#sample-ingestion-and-profiling-for-snowflake","title":"Sample ingestion and profiling for Snowflake","text":"<p>Let's demonstrate how you can profile your data and add data quality tests for a Snowflake data warehouse using Prefect. </p>"},{"location":"all_workflows/#install-sqlalchemy-package-for-your-connector","title":"Install SQLAlchemy package for your connector","text":"<p>First, make sure that you have the <code>prefect-openmetadata</code> and the <code>snowflake-sqlalchemy</code> packages installed in your environment.  </p>"},{"location":"all_workflows/#test-the-connection","title":"Test the connection","text":"<p>You can test the connection to your end destination using the following Prefect flow:</p> <pre><code>from prefect_openmetadata.flows import validate_connection\n\nconfig = \"\"\"\nconnection:\n  config:\n    type: Snowflake\n    username: DEMO\n    password: xxx\n    account: xxx.us-east-2.aws\n    database: YOUR_DB\n    warehouse: COMPUTE_WH\nconnectionType: Database\n\"\"\"\n\n\nif __name__ == \"__main__\":\n    validate_connection(config)\n</code></pre>"},{"location":"all_workflows/#ingest-your-metadata","title":"Ingest your metadata","text":"<p>You can use the configuration below as a template - make sure to adjust it to match your username, password, your database name, and further details about the tables and schemas you want to include. You may optionally ingest your dbt manifest and dbt catalog to display dbt models and dbt docs along with your tables and views.</p> <pre><code>from prefect_openmetadata.flows import ingest_metadata\n\nconfig = \"\"\"\nsource:\n  type: snowflake\n  serviceName: snowflake\n  serviceConnection:\n    config:\n      type: Snowflake\n      username: \"xxx\"\n      password: \"xxx\"\n      database: \"YOURDB\"\n      warehouse: \"COMPUTE_WH\"\n      account: \"YOUR_ACCOUNT.us-east-2.aws\"\n  sourceConfig:\n    config:\n      type: DatabaseMetadata\n      schemaFilterPattern:\n        includes: \n            - YOUR_SCHEMA\n      dbtConfigSource:\n        dbtCatalogFilePath: /Users/you/catalog.json\n        dbtManifestFilePath: /Users/you/manifest.json\nsink:\n  type: metadata-rest\n  config: {}\nworkflowConfig:\n  loggerLevel: DEBUG\n  openMetadataServerConfig:\n    hostPort: http://localhost:8585/api\n    authProvider: no-auth\n\"\"\"\n\nif __name__ == \"__main__\":\n    ingest_metadata(config)\n</code></pre>"},{"location":"all_workflows/#profile-your-data-and-add-data-quality-tests","title":"Profile your data and add data quality tests","text":"<p>Here is a flow to profile the previously ingested data:</p> <pre><code>from prefect_openmetadata.flows import profile_metadata\n\nconfig = \"\"\"\nsource:\n  type: snowflake\n  serviceName: snowflake\n  serviceConnection:\n    config:\n      type: Snowflake\n      username: \"xxx\"\n      password: \"xxx\"\n      database: \"YOURDB\"\n      warehouse: \"COMPUTE_WH\"\n      account: \"YOUR_ACCOUNT.us-east-2.aws\"\n  sourceConfig:\n    config:\n      type: Profiler\n      fqnFilterPattern:\n        includes:\n        - snowflake.ANNA.ANNA_DEMO.*\n      generateSampleData: true\nprocessor:\n  type: orm-profiler\n  config:\n    test_suite:\n      name: demo_data\n      tests:\n        - table: snowflake.ANNA.ANNA_DEMO.RAW_CUSTOMERS\n          table_tests:\n            - testCase:\n                config:\n                  value: 100\n                tableTestType: tableRowCountToEqual\n          column_tests:\n            - columnName: ID\n              testCase:\n                config:\n                  minValue: 1\n                  maxValue: 100\n                columnTestType: columnValuesToBeBetween\nsink:\n  type: metadata-rest\n  config: {}\nworkflowConfig:\n  openMetadataServerConfig:\n    hostPort: http://localhost:8585/api\n    authProvider: no-auth\n\"\"\"\n\nif __name__ == \"__main__\":\n    profile_metadata(config)\n</code></pre>"},{"location":"all_workflows/#add-usage-query-patterns","title":"Add usage query patterns","text":"<p>Finally, to ensure that you can see sample data and queries performed over given assets, run the following usage workflow (as always, adjust to your needs):</p> <pre><code>from prefect_openmetadata.flows import ingest_metadata\n\nconfig = \"\"\"\nsource:\n  type: type: snowflake-usage\n  serviceName: snowflake\n  serviceConnection:\n    config:\n      type: Snowflake\n      username: \"xxx\"\n      password: \"xxx\"\n      database: \"YOURDB\"\n      warehouse: \"COMPUTE_WH\"\n      account: \"YOUR_ACCOUNT.us-east-2.aws\"\n  sourceConfig:\n    config:\n      type: DatabaseUsage\nprocessor:\n  type: query-parser\n  config:\n    filter: ''\nworkflowConfig:\n  openMetadataServerConfig:\n    hostPort: http://localhost:8585/api\n    authProvider: no-auth\n\"\"\"\n\nif __name__ == \"__main__\":\n    ingest_metadata(config)\n</code></pre> <p>Congratulations on building your first metadata ingestion workflows with OpenMetadata and Prefect! Head over to the next section to see how you can run this flow on schedule. </p>"},{"location":"flows/","title":"Flows & tasks","text":""},{"location":"flows/#prefect_openmetadata.flows--prefect-flows-and-tasks-for-openmetadata-workflows-include","title":"Prefect flows and tasks for OpenMetadata workflows include:","text":"<ul> <li>metadata ingestion workflow: information about your tables, view, schemas, etc</li> <li>usage ingestion workflow: query and audit logs showing usage patterns</li> <li>profiler workflow: profile data and run data validation tests</li> </ul> <p>Follow the main documentation for guidance on:</p> <ul> <li>installing and configuring <code>Prefect</code> and <code>OpenMetadata</code>,</li> <li>running metadata ingestion flows locally and on schedule.</li> </ul>"},{"location":"flows/#prefect_openmetadata.flows.OpenMetadataFailedConnection","title":"<code>OpenMetadataFailedConnection</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception for failed connection attempts</p> Source code in <code>prefect_openmetadata/flows.py</code> <pre><code>class OpenMetadataFailedConnection(Exception):\n    \"\"\"Exception for failed connection attempts\"\"\"\n</code></pre>"},{"location":"flows/#prefect_openmetadata.flows.ingest_metadata","title":"<code>ingest_metadata(config, is_json=False)</code>","text":"<p>Ingests raw metadata about tables, dashboards, users, topics, pipelines, etc. The same flow is used for usage ingestion.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>configuration spec, by default in YAML, optionally in JSON</p> required <code>is_json</code> <code>bool</code> <p>flag whether <code>config</code> is a JSON spec rather than YAML</p> <code>False</code> <p>Examples:</p> <p>Flow ingesting metadata using Prefect: <pre><code>from prefect_openmetadata.flows import ingest_metadata\n\nconfig = \"\"\"See an example in the section: Run ingestion flow\"\"\"\n\nif __name__ == \"__main__\":\n    ingest_metadata(config)\n</code></pre></p> Source code in <code>prefect_openmetadata/flows.py</code> <pre><code>@flow\ndef ingest_metadata(config: str, is_json: bool = False) -&gt; None:\n    \"\"\"\n    Ingests raw metadata about tables, dashboards, users, topics, pipelines, etc.\n    The same flow is used for usage ingestion.\n\n    Args:\n        config: configuration spec, by default in YAML, optionally in JSON\n        is_json: flag whether `config` is a JSON spec rather than YAML\n\n    Examples:\n        Flow ingesting metadata using Prefect:\n        ```python\n        from prefect_openmetadata.flows import ingest_metadata\n\n        config = \\\"\"\"See an example in the section: Run ingestion flow\\\"\"\"\n\n        if __name__ == \"__main__\":\n            ingest_metadata(config)\n        ```\n    \"\"\"\n    om_workflow_config = json.loads(config) if is_json else yaml.safe_load(config)\n    om_workflow_model = OpenMetadataWorkflowConfig.parse_obj(om_workflow_config)\n    run_ingestion_task(om_workflow_model)\n</code></pre>"},{"location":"flows/#prefect_openmetadata.flows.make_test_connection","title":"<code>make_test_connection(conn_config)</code>  <code>async</code>","text":"<p>Task making a test connection to the specified OpenMetadata connection</p> <p>Parameters:</p> Name Type Description Default <code>conn_config</code> <code>TestServiceConnectionRequest</code> <p>connection spec as a pydantic model</p> required Source code in <code>prefect_openmetadata/flows.py</code> <pre><code>@task\nasync def make_test_connection(conn_config: TestServiceConnectionRequest) -&gt; None:\n    \"\"\"\n    Task making a test connection to the specified OpenMetadata connection\n\n    Args:\n        conn_config: connection spec as a pydantic model\n    \"\"\"\n    logger = get_run_logger()\n    connection = get_connection(conn_config.connection.config)\n    try:\n        test_connection(connection)\n        logger.info(\"Connection successful!\")\n    except OpenMetadataFailedConnection as exc:\n        logger.error(\"Test connection failed\")\n        raise exc\n</code></pre>"},{"location":"flows/#prefect_openmetadata.flows.profile_metadata","title":"<code>profile_metadata(config, is_json=False)</code>","text":"<p>Profiles metadata about tables, dashboards, users, topics, pipelines, etc.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>configuration spec, by default in YAML, optionally in JSON</p> required <code>is_json</code> <code>bool</code> <p>flag whether <code>config</code> is a JSON spec rather than YAML</p> <code>False</code> <p>Examples:</p> <p>Flow profiling metadata using Prefect: <pre><code>from prefect_openmetadata.flows import profile_metadata\n\nconfig = \"\"\"See an example in the section: Run profiling flow\"\"\"\n\nif __name__ == \"__main__\":\n    profile_metadata(config)\n</code></pre></p> Source code in <code>prefect_openmetadata/flows.py</code> <pre><code>@flow\ndef profile_metadata(config: str, is_json: bool = False) -&gt; None:\n    \"\"\"\n    Profiles metadata about tables, dashboards, users, topics, pipelines, etc.\n\n    Args:\n        config: configuration spec, by default in YAML, optionally in JSON\n        is_json: flag whether `config` is a JSON spec rather than YAML\n\n    Examples:\n        Flow profiling metadata using Prefect:\n        ```python\n        from prefect_openmetadata.flows import profile_metadata\n\n        config = \\\"\"\"See an example in the section: Run profiling flow\\\"\"\"\n\n        if __name__ == \"__main__\":\n            profile_metadata(config)\n        ```\n    \"\"\"\n    om_workflow_config = json.loads(config) if is_json else yaml.safe_load(config)\n    om_workflow_model = OpenMetadataWorkflowConfig.parse_obj(om_workflow_config)\n    run_profiling_task(om_workflow_model)\n</code></pre>"},{"location":"flows/#prefect_openmetadata.flows.run_ingestion_task","title":"<code>run_ingestion_task(om_workflow_model)</code>  <code>async</code>","text":"<p>Task ingesting metadata into OpenMetadata backend</p> <p>Parameters:</p> Name Type Description Default <code>om_workflow_model</code> <code>OpenMetadataWorkflowConfig</code> <p>ingestion spec as a pydantic model</p> required Source code in <code>prefect_openmetadata/flows.py</code> <pre><code>@task\nasync def run_ingestion_task(om_workflow_model: OpenMetadataWorkflowConfig) -&gt; None:\n    \"\"\"\n    Task ingesting metadata into OpenMetadata backend\n\n    Args:\n        om_workflow_model: ingestion spec as a pydantic model\n    \"\"\"\n    workflow = PrefectOpenMetadataIngestion(om_workflow_model)\n    workflow.execute()\n    workflow.log_flow_status()\n    workflow.raise_from_status()\n    workflow.stop()\n</code></pre>"},{"location":"flows/#prefect_openmetadata.flows.run_profiling_task","title":"<code>run_profiling_task(om_workflow_model)</code>  <code>async</code>","text":"<p>Task profiling a given OpenMetadata source</p> <p>Parameters:</p> Name Type Description Default <code>om_workflow_model</code> <code>OpenMetadataWorkflowConfig</code> <p>profiling spec as a pydantic model</p> required Source code in <code>prefect_openmetadata/flows.py</code> <pre><code>@task\nasync def run_profiling_task(om_workflow_model: OpenMetadataWorkflowConfig) -&gt; None:\n    \"\"\"\n    Task profiling a given OpenMetadata source\n\n    Args:\n        om_workflow_model: profiling spec as a pydantic model\n    \"\"\"\n    workflow = PrefectOpenMetadataProfiler(om_workflow_model)\n    workflow.execute()\n    workflow.log_flow_status()\n    workflow.raise_from_status()\n    workflow.stop()\n</code></pre>"},{"location":"flows/#prefect_openmetadata.flows.validate_connection","title":"<code>validate_connection(conn_config, is_json=False)</code>","text":"<p>Makes a SQLAlchemy connection based on a given JSON or YAML config for testing. Go to the OpenMetadata schema definitions, to inspect required fields to connect with your desired system and crawl its metadata.</p> <p>Requires installing the required sqlalchemy subpackage for the relevant connector e.g. Snowflake connection requires <code>pip install snowflake-sqlalchemy</code></p> <p>Parameters:</p> Name Type Description Default <code>conn_config</code> <code>str</code> <p>connection spec as a string from a JSON spec</p> required <code>is_json</code> <code>bool</code> <p>flag whether <code>conn_config</code> is a JSON spec rather than YAML</p> <code>False</code> <p>Examples:</p> <p>Flow testing connection using Prefect: <pre><code>from prefect_openmetadata.flows import validate_connection\n\nconfig = \"\"\"\nconnection:\n  config:\n    type: Snowflake\n    username: DEMO\n    password: xxx\n    account: xxx.us-east-2.aws\n    database: YOUR_DB\n    warehouse: COMPUTE_WH\nconnectionType: Database\n\"\"\"\n\nif __name__ == \"__main__\":\n    validate_connection(config)\n</code></pre></p> Source code in <code>prefect_openmetadata/flows.py</code> <pre><code>@flow\ndef validate_connection(conn_config: str, is_json: bool = False):\n    \"\"\"\n    Makes a SQLAlchemy connection based on a given JSON or YAML config for testing.\n    Go to the [OpenMetadata schema definitions](https://github.com/open-metadata/OpenMetadata/tree/main/catalog-rest-service/src/main/resources/json/schema/entity/services/connections),\n    to inspect required fields to connect with your desired system and crawl its metadata.\n\n    Requires installing the required sqlalchemy subpackage for the relevant connector\n    e.g. Snowflake connection requires `pip install snowflake-sqlalchemy`\n\n    Args:\n        conn_config: connection spec as a string from a JSON spec\n        is_json:  flag whether `conn_config` is a JSON spec rather than YAML\n\n    Examples:\n        Flow testing connection using Prefect:\n        ```python\n        from prefect_openmetadata.flows import validate_connection\n\n        config = \\\"\"\"\n        connection:\n          config:\n            type: Snowflake\n            username: DEMO\n            password: xxx\n            account: xxx.us-east-2.aws\n            database: YOUR_DB\n            warehouse: COMPUTE_WH\n        connectionType: Database\n        \\\"\"\"\n\n        if __name__ == \"__main__\":\n            validate_connection(config)\n        ```\n    \"\"\"  # noqa\n    conn_spec_dict = json.loads(conn_config) if is_json else yaml.safe_load(conn_config)\n    conn_model = TestServiceConnectionRequest.parse_obj(conn_spec_dict)\n    make_test_connection(conn_model)\n</code></pre>"},{"location":"ingestion_workflow/","title":"Ingestion workflow module","text":"<p>Extension to the OpenMetadata Workflow class</p>"},{"location":"ingestion_workflow/#prefect_openmetadata.ingestion_workflow.PrefectOpenMetadataIngestion","title":"<code>PrefectOpenMetadataIngestion</code>","text":"<p>             Bases: <code>Workflow</code></p> <p>OpenMetadata ingestion workflow that adds a method allowing to log the workflow status to the Prefect backend.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>string with a YAML or JSON configuration file</p> required Source code in <code>prefect_openmetadata/ingestion_workflow.py</code> <pre><code>class PrefectOpenMetadataIngestion(Workflow):\n    \"\"\"\n    OpenMetadata ingestion workflow that adds a method\n    allowing to log the workflow status to the Prefect backend.\n\n    Args:\n         config: string with a YAML or JSON configuration file\n    \"\"\"\n\n    def __int__(self, config: OpenMetadataWorkflowConfig):\n        \"\"\"\n        Args:\n            config: string with a YAML or JSON configuration file\n        \"\"\"\n        super().__init__(config=config)\n\n    def log_flow_status(self) -&gt; None:\n        \"\"\"\n        Log workflow status to the Prefect API backend\n        \"\"\"\n        logger = get_run_logger()\n        logger.info(\"Source Status: %s\", self.source.get_status().as_string())\n        if hasattr(self, \"stage\"):\n            logger.info(\"Stage Status: %s\", self.stage.get_status().as_string())\n        if hasattr(self, \"sink\"):\n            logger.info(\"Sink Status: %s\", self.sink.get_status().as_string())\n        if hasattr(self, \"bulk_sink\"):\n            logger.info(\"Bulk Sink Status: %s\", self.bulk_sink.get_status().as_string())\n</code></pre>"},{"location":"ingestion_workflow/#prefect_openmetadata.ingestion_workflow.PrefectOpenMetadataIngestion.__int__","title":"<code>__int__(config)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>config</code> <code>OpenMetadataWorkflowConfig</code> <p>string with a YAML or JSON configuration file</p> required Source code in <code>prefect_openmetadata/ingestion_workflow.py</code> <pre><code>def __int__(self, config: OpenMetadataWorkflowConfig):\n    \"\"\"\n    Args:\n        config: string with a YAML or JSON configuration file\n    \"\"\"\n    super().__init__(config=config)\n</code></pre>"},{"location":"ingestion_workflow/#prefect_openmetadata.ingestion_workflow.PrefectOpenMetadataIngestion.log_flow_status","title":"<code>log_flow_status()</code>","text":"<p>Log workflow status to the Prefect API backend</p> Source code in <code>prefect_openmetadata/ingestion_workflow.py</code> <pre><code>def log_flow_status(self) -&gt; None:\n    \"\"\"\n    Log workflow status to the Prefect API backend\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Source Status: %s\", self.source.get_status().as_string())\n    if hasattr(self, \"stage\"):\n        logger.info(\"Stage Status: %s\", self.stage.get_status().as_string())\n    if hasattr(self, \"sink\"):\n        logger.info(\"Sink Status: %s\", self.sink.get_status().as_string())\n    if hasattr(self, \"bulk_sink\"):\n        logger.info(\"Bulk Sink Status: %s\", self.bulk_sink.get_status().as_string())\n</code></pre>"},{"location":"install_openmetadata/","title":"Install OpenMetadata","text":""},{"location":"install_openmetadata/#requirements","title":"Requirements","text":"<p>Please ensure your host system meets the requirements listed below:</p> <ul> <li>Python 3.8+</li> <li>Docker 20.10.0+</li> <li>Docker Compose</li> </ul>"},{"location":"install_openmetadata/#installing-openmetadata","title":"Installing OpenMetadata","text":""},{"location":"install_openmetadata/#clone-the-prefect-openmetadata-repository","title":"Clone the <code>prefect-openmetadata</code> repository","text":"<p>First, clone the latest version of the prefect-openmetadata Prefect Collection. </p> <p>Then, navigate to the directory <code>openmetadata-docker</code> containing the <code>docker-compose.yml</code> file with the minimal requirements to get started with OpenMetadata.</p>"},{"location":"install_openmetadata/#start-openmetadata-containers","title":"Start OpenMetadata containers","text":"<p>You can start the containers with OpenMetadata components using:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will create a docker network and containers with the following services:</p> <ul> <li><code>openmetadata_mysql</code> - metadata store that serves as a persistence layer holding your metadata,</li> <li><code>openmetadata_elasticsearch</code> - indexing service to search the metadata catalog,</li> <li><code>openmetadata_server</code> - the OpenMetadata UI and API server allowing you to discover insights and interact with your metadata.</li> </ul> <p>Wait a couple of minutes until the setup is finished.</p> <p>To check the status of all services, you may run the <code>docker compose ps</code> command to investigate the status of all Docker containers:</p> <pre><code>NAME                         COMMAND                  SERVICE               STATUS              PORTS\nopenmetadata_elasticsearch   \"/tini -- /usr/local\u2026\"   elasticsearch         running             0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9300-&gt;9300/tcp\nopenmetadata_mysql           \"/entrypoint.sh mysq\u2026\"   mysql                 running (healthy)   33060-33061/tcp\nopenmetadata_server          \"./openmetadata-star\u2026\"   openmetadata-server   running             0.0.0.0:8585-&gt;8585/tcp\n</code></pre>"},{"location":"install_openmetadata/#confirm-you-can-access-the-openmetadata-ui","title":"Confirm you can access the OpenMetadata UI","text":"<p>Visit the following URL to confirm you can access the UI and start exploring OpenMetadata:</p> <pre><code>http://localhost:8585\n</code></pre> <p>Login with username <code>admin</code> and password <code>admin</code>. </p> <p>You should see a page similar to the following as the landing page for the OpenMetadata UI.</p> <p></p>"},{"location":"install_openmetadata/#why-should-you-use-prefect-for-metadata-ingestion","title":"Why should you use Prefect for metadata ingestion?","text":"<p>The challenge with the metadata ingestion is to ensure that this process can be automated and can run reliably, either on a regular interval, or ad-hoc. This is where Prefect can help.</p> <p>Prefect is a general-purpose orchestration and dataflow coordination platform allowing you to build, run, schedule, and operationalize your dataflow at scale. It supports both batch and streaming workflows and provides an excellent developer experience allowing you to run your flows locally and seamlessly move to production and to Cloud when you\u2019re ready.</p> <p>Among many other features, it natively supports:</p> <ul> <li>dynamic runtime-discoverable and modular workflows,</li> <li>passing data between tasks,</li> <li>running your workflows on various execution platforms (on-prem, cloud, Docker, Kubernetes, serverless containers, and more) while maintaining privacy via a hybrid execution model,</li> <li>scaling out for parallel and concurrent execution with async, Dask, and Ray,</li> <li>various integrations through Prefect Collections - such as this one!</li> </ul>"},{"location":"install_openmetadata/#install-prefect","title":"Install Prefect","text":"<p>You can install Prefect and other dependencies needed for this integration using the command (run this from the home directory which <code>prefect-openmetadata</code> was cloned into):</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>To make sure that OpenMetadata logs are stored in the Prefect backend and displayed in the UI, run the following command: <pre><code>prefect config set PREFECT_LOGGING_EXTRA_LOGGERS='Ingestion,OMetaAPI,Metadata,Profiler,Utils'\n</code></pre></p> <p>This way, Prefect is aware of the extra loggers from OpenMetadata.</p>"},{"location":"install_openmetadata/#self-hosted-orion-server","title":"Self-hosted Orion server","text":"<p>When you install Prefect, this will not only install the client library, but also an ephemeral API server and UI, which can be optionally started using:</p> <pre><code>prefect orion start\n</code></pre> <p>If you navigate to the URL, you\u2019ll be able to access a locally running Prefect Orion UI:</p> <pre><code>http://localhost:4200\n</code></pre>"},{"location":"install_openmetadata/#prefect-cloud","title":"Prefect Cloud","text":"<p>Alternatively, you can sign up for a free Prefect Cloud account and create a workspace.  Then, install Prefect and log into your Cloud workspace from a terminal:</p> <pre><code>pip install prefect\nprefect cloud login\n</code></pre>"},{"location":"profiler_workflow/","title":"Profiler workflow module","text":"<p>Extension to the OpenMetadata ProfilerWorkflow class</p>"},{"location":"profiler_workflow/#prefect_openmetadata.profiler_workflow.PrefectOpenMetadataProfiler","title":"<code>PrefectOpenMetadataProfiler</code>","text":"<p>             Bases: <code>ProfilerWorkflow</code></p> <p>OpenMetadata profiler workflow that adds a method allowing to log the workflow status to the Prefect backend.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>string with a YAML or JSON configuration file</p> required Source code in <code>prefect_openmetadata/profiler_workflow.py</code> <pre><code>class PrefectOpenMetadataProfiler(ProfilerWorkflow):\n    \"\"\"\n    OpenMetadata profiler workflow that adds a method\n    allowing to log the workflow status to the Prefect backend.\n\n    Args:\n         config: string with a YAML or JSON configuration file\n    \"\"\"\n\n    def __int__(self, config: OpenMetadataWorkflowConfig):\n        \"\"\"\n        Args:\n            config: string with a YAML or JSON configuration file\n        \"\"\"\n        super().__init__(config=config)\n\n    def log_flow_status(self) -&gt; None:\n        \"\"\"\n        Log workflow status to the Prefect API backend\n        \"\"\"\n        logger = get_run_logger()\n        logger.info(\"Source Status: %s\", self.source_status.as_string())\n        logger.info(\"Processir Status: %s\", self.processor.get_status().as_string())\n        if hasattr(self, \"sink\"):\n            logger.info(\"Sink Status: %s\", self.sink.get_status().as_string())\n</code></pre>"},{"location":"profiler_workflow/#prefect_openmetadata.profiler_workflow.PrefectOpenMetadataProfiler.__int__","title":"<code>__int__(config)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>config</code> <code>OpenMetadataWorkflowConfig</code> <p>string with a YAML or JSON configuration file</p> required Source code in <code>prefect_openmetadata/profiler_workflow.py</code> <pre><code>def __int__(self, config: OpenMetadataWorkflowConfig):\n    \"\"\"\n    Args:\n        config: string with a YAML or JSON configuration file\n    \"\"\"\n    super().__init__(config=config)\n</code></pre>"},{"location":"profiler_workflow/#prefect_openmetadata.profiler_workflow.PrefectOpenMetadataProfiler.log_flow_status","title":"<code>log_flow_status()</code>","text":"<p>Log workflow status to the Prefect API backend</p> Source code in <code>prefect_openmetadata/profiler_workflow.py</code> <pre><code>def log_flow_status(self) -&gt; None:\n    \"\"\"\n    Log workflow status to the Prefect API backend\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Source Status: %s\", self.source_status.as_string())\n    logger.info(\"Processir Status: %s\", self.processor.get_status().as_string())\n    if hasattr(self, \"sink\"):\n        logger.info(\"Sink Status: %s\", self.sink.get_status().as_string())\n</code></pre>"},{"location":"run_ingestion_flow/","title":"Run metadata ingestion","text":""},{"location":"run_ingestion_flow/#configure-your-ingestion-spec","title":"Configure your ingestion spec","text":"<p>Here is a simple flow example demonstratig how you can ingest data from a Postgres database into your <code>OpenMetadata</code> backend using Prefect: example_postgres_ingestion.py.</p> <pre><code>from prefect_openmetadata.flows import ingest_metadata\n\npostgres = \"\"\"\nsource:\n  type: postgres\n  serviceName: local_postgres\n  serviceConnection:\n    config:\n      type: Postgres\n      username: postgres\n      password: postgres\n      hostPort: localhost:5432\n  sourceConfig:\n    config:\n      markDeletedTables: true\n      includeTables: true\n      includeViews: false\nsink:\n  type: metadata-rest\n  config: {}\nworkflowConfig:\n  openMetadataServerConfig:\n    hostPort: http://localhost:8585/api\n    authProvider: openmetadata\n    securityConfig:\n      jwtToken: 'your_token'\n\"\"\"\n\nif __name__ == \"__main__\":\n    ingest_metadata(postgres)\n</code></pre> <p>Each flow is based on a configuration YAML or JSON spec that follows similar structure to the following:</p> <pre><code>source:\n  type: sample-data\n  serviceName: sample_data\n  serviceConnection:\n    config:\n      type: SampleData\n      sampleDataFolder: \"example-data\"\n  sourceConfig: {}\nsink:\n  type: metadata-rest\n  config: {}\nworkflowConfig:\n  openMetadataServerConfig:\n    hostPort: http://localhost:8585/api\n    authProvider: no-auth\n</code></pre>"},{"location":"run_ingestion_flow/#run-ingestion-workflow-locally","title":"Run ingestion workflow locally","text":"<p>Now you can paste the config from above as a string into your flow and run it:</p> <pre><code>from prefect_openmetadata.flows import ingest_metadata\n\nconfig = \"\"\"\nYOUR_CONFIG\n\"\"\"\n\nif __name__ == \"__main__\":\n    ingest_metadata(config)\n</code></pre> <p>After running your flow, you should see new users, datasets, dashboards, or similar metadata objects in your OpenMetadata UI. Also, your Prefect UI will display the workflow run and will show the logs with details on which source system has been scanned and which data has been ingested.</p> <p>Congratulations on building your first metadata ingestion workflow with OpenMetadata and Prefect! Head over to the next section to see how you can run this flow on schedule. </p>"},{"location":"schedule_ingestion_flow/","title":"Schedule and deploy metadata ingestion flows","text":""},{"location":"schedule_ingestion_flow/#schedule-your-openmetadata-ingestion-flows-with-prefect","title":"Schedule your OpenMetadata ingestion flows with Prefect","text":"<p>Ingesting your data via manually executed scripts is great for initial exploration, but in order to build a reliable metadata platform, you need to run those workflows on a regular cadence. That\u2019s where you can leverage Prefect schedules and deployments.</p> <p>Here is how you can create a <code>Deployment</code> to ensure that your metadata ingestion flow <code>ingest_metadata</code> in the script <code>myflow.py</code> gets refreshed every 15 minutes:</p> <pre><code>prefect deployment build -a -n dev myflow.py:ingest_metadata --interval 900\n</code></pre> <p>Here is an explanation of the <code>prefect deployment build</code> flags:</p> <ul> <li><code>-a</code> - will automatically register the resulting deployment with the API</li> <li><code>-n</code> - specifies the name of the deployment - you could use it to differentiate between a deployment for development and production environment</li> <li><code>myflow.py:ingest_metadata</code> - entrypoint to the flow object, i.e. the Python script and the flow function name</li> <li><code>--interval 900</code> - schedule interval in seconds, here: triggering a new flow run every 15 minutes. With the asynchronous scheduling service in Prefect 2.0, you can schedule your flow to run even every 10 seconds if you need your metadata to be updated near real-time.</li> </ul>"},{"location":"schedule_ingestion_flow/#deploy-your-execution-layer-to-run-your-flows","title":"Deploy your execution layer to run your flows","text":"<p>So far, we\u2019ve looked at how you can create and schedule your workflow, but where does this code actually run? This is a place where the concepts of storage and infrastructure blocks, as well as work queues and agents become important. But don\u2019t worry - all you need to know to get started is running a single CLI command:</p> <pre><code>prefect agent start -q default\n</code></pre> <p>Once you have executed that command, your scheduled deployments (such as the one we defined using <code>myflow.py</code> above) are now scheduled, and Prefect will ensure that your metadata stays up-to-date.</p> <p>You can observe the state of your metadata ingestion workflows from the Prefect Orion UI. The UI will also include detailed logs showing which metadata got updated to ensure your data platform remains healthy and observable.</p>"},{"location":"schedule_ingestion_flow/#using-prefect-in-the-cloud","title":"Using Prefect in the Cloud","text":"<p>If you want to move beyond this local installation, you can deploy Prefect to run your OpenMetadata ingestion workflows by:</p> <ul> <li>self-hosting the orchestration layer - see the list of resources on Prefect Discourse,</li> <li>or signing up for Prefect Cloud - the following page will walk you through the process.</li> </ul> <p>For various deployment options of OpenMetadata, check the Deployment documentation.</p>"},{"location":"schedule_ingestion_flow/#questions-about-using-openmetadata-with-prefect","title":"Questions about using OpenMetadata with Prefect","text":"<p>If you have any questions about configuring Prefect, post your question on Prefect Discourse or in the Prefect Community Slack.</p> <p>And if you need support for OpenMetadata, get in touch on OpenMetadata Slack.</p>"}]}