{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"prefect-openmetadata Welcome! Using Prefect and OpenMetadata together will help you build and maintain a data platform you can trust . Prefect allows you to orchestrate your data workflows and provides visibility into the health of your workflow execution and workflow lineage . With OpenMetadata integration, you can enrich your orchestration system with metadata about data lineage, data catalog, data quality and governance, giving you a single pane of glass about the health of your system. Getting Started Python setup Requires an installation of Python 3.8+. We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv. This prefect-openmetadata package is designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the documentation . Installation Install prefect-openmetadata with pip : pip install prefect-openmetadata Install OpenMetadata and Prefect 2.0 Head over to the install OpenMetadata page for detailed instructions on how to install and configure both platforms. Write and run metadata ingestion flow from prefect_openmetadata.flows import ingest_metadata config = \"\"\"See an example in the section: Run ingestion flow\"\"\" if __name__ == \"__main__\" : ingest_metadata ( config ) For more details, check the run ingestion flow section. Schedule a metadata ingestion flow Simple example: DeploymentSpec ( name = \"openmetadata-dev\" , flow = ingest_metadata , schedule = IntervalSchedule ( interval = timedelta ( minutes = 15 )), ) For more details, check the schedule ingestion flow section. Resources If you encounter any bugs while using prefect-openmetadata , feel free to open an issue in the prefect-openmetadata repository. If you have any questions or issues while using prefect-openmetadata , you can find help in either the Prefect Discourse forum or the Prefect Slack community . Development If you'd like to install a version of prefect-openmetadata for development, clone the repository and perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-openmetadata.git cd prefect-openmetadata/ pip install -e \".[dev]\" # Install linting pre-commit hooks pre-commit install","title":"Home"},{"location":"#prefect-openmetadata","text":"","title":"prefect-openmetadata"},{"location":"#welcome","text":"Using Prefect and OpenMetadata together will help you build and maintain a data platform you can trust . Prefect allows you to orchestrate your data workflows and provides visibility into the health of your workflow execution and workflow lineage . With OpenMetadata integration, you can enrich your orchestration system with metadata about data lineage, data catalog, data quality and governance, giving you a single pane of glass about the health of your system.","title":"Welcome!"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#python-setup","text":"Requires an installation of Python 3.8+. We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv. This prefect-openmetadata package is designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the documentation .","title":"Python setup"},{"location":"#installation","text":"Install prefect-openmetadata with pip : pip install prefect-openmetadata","title":"Installation"},{"location":"#install-openmetadata-and-prefect-20","text":"Head over to the install OpenMetadata page for detailed instructions on how to install and configure both platforms.","title":"Install OpenMetadata and Prefect 2.0"},{"location":"#write-and-run-metadata-ingestion-flow","text":"from prefect_openmetadata.flows import ingest_metadata config = \"\"\"See an example in the section: Run ingestion flow\"\"\" if __name__ == \"__main__\" : ingest_metadata ( config ) For more details, check the run ingestion flow section.","title":"Write and run metadata ingestion flow"},{"location":"#schedule-a-metadata-ingestion-flow","text":"Simple example: DeploymentSpec ( name = \"openmetadata-dev\" , flow = ingest_metadata , schedule = IntervalSchedule ( interval = timedelta ( minutes = 15 )), ) For more details, check the schedule ingestion flow section.","title":"Schedule a metadata ingestion flow"},{"location":"#resources","text":"If you encounter any bugs while using prefect-openmetadata , feel free to open an issue in the prefect-openmetadata repository. If you have any questions or issues while using prefect-openmetadata , you can find help in either the Prefect Discourse forum or the Prefect Slack community .","title":"Resources"},{"location":"#development","text":"If you'd like to install a version of prefect-openmetadata for development, clone the repository and perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-openmetadata.git cd prefect-openmetadata/ pip install -e \".[dev]\" # Install linting pre-commit hooks pre-commit install","title":"Development"},{"location":"all_workflows/","text":"Run metadata ingestion, usage workflow and data profiler for Snowflake This demo will walk you through the process of creating ingestion, usage and profiling workflows. Configure your profiling spec OpenMetadata heavily relies on schemas. You need to understand which arguments are required and which are optional, and which are important for your use case. The easiest way to identify the required and optional arguments of any ingestion or profiling workflow is to import the OpenMetadataWorkflowConfig . From this pydantic model, you can drill down into other components using your favorite IDE e.g. Pycharm or VSCode. From there, you can again drill down into Source , SourceConfig , Sink , etc. from metadata.generated.schema.metadataIngestion.workflow import ( OpenMetadataWorkflowConfig , Source , SourceConfig , Sink ) Sample ingestion and profiling for Snowflake Let's demonstrate how you can profile your data and add data quality tests for a Snowflake data warehouse using Prefect. Install SQLAlchemy package for your connector First, make sure that you have the prefect-openmetadata and the snowflake-sqlalchemy packages installed in your environment. Test the connection You can test the connection to your end destination using the following Prefect flow: from prefect_openmetadata.flows import validate_connection config = \"\"\" connection: config: type: Snowflake username: DEMO password: xxx account: xxx.us-east-2.aws database: YOUR_DB warehouse: COMPUTE_WH connectionType: Database \"\"\" if __name__ == \"__main__\" : validate_connection ( config ) Ingest your metadata You can use the configuration below as a template - make sure to adjust it to match your username, password, your database name, and further details about the tables and schemas you want to include. You may optionally ingest your dbt manifest and dbt catalog to display dbt models and dbt docs along with your tables and views. from prefect_openmetadata.flows import ingest_metadata config = \"\"\" source: type: snowflake serviceName: snowflake serviceConnection: config: type: Snowflake username: \"xxx\" password: \"xxx\" database: \"YOURDB\" warehouse: \"COMPUTE_WH\" account: \"YOUR_ACCOUNT.us-east-2.aws\" sourceConfig: config: type: DatabaseMetadata schemaFilterPattern: includes: - YOUR_SCHEMA dbtConfigSource: dbtCatalogFilePath: /Users/you/catalog.json dbtManifestFilePath: /Users/you/manifest.json sink: type: metadata-rest config: {} workflowConfig: loggerLevel: DEBUG openMetadataServerConfig: hostPort: http://localhost:8585/api authProvider: no-auth \"\"\" if __name__ == \"__main__\" : ingest_metadata ( config ) Profile your data and add data quality tests Here is a flow to profile the previously ingested data: from prefect_openmetadata.flows import profile_metadata config = \"\"\" source: type: snowflake serviceName: snowflake serviceConnection: config: type: Snowflake username: \"xxx\" password: \"xxx\" database: \"YOURDB\" warehouse: \"COMPUTE_WH\" account: \"YOUR_ACCOUNT.us-east-2.aws\" sourceConfig: config: type: Profiler fqnFilterPattern: includes: - snowflake.ANNA.ANNA_DEMO.* generateSampleData: true processor: type: orm-profiler config: test_suite: name: demo_data tests: - table: snowflake.ANNA.ANNA_DEMO.RAW_CUSTOMERS table_tests: - testCase: config: value: 100 tableTestType: tableRowCountToEqual column_tests: - columnName: ID testCase: config: minValue: 1 maxValue: 100 columnTestType: columnValuesToBeBetween sink: type: metadata-rest config: {} workflowConfig: openMetadataServerConfig: hostPort: http://localhost:8585/api authProvider: no-auth \"\"\" if __name__ == \"__main__\" : profile_metadata ( config ) Add usage query patterns Finally, to ensure that you can see sample data and queries performed over given assets, run the following usage workflow ( as always, adjust to your needs ): from prefect_openmetadata.flows import ingest_metadata config = \"\"\" source: type: type: snowflake-usage serviceName: snowflake serviceConnection: config: type: Snowflake username: \"xxx\" password: \"xxx\" database: \"YOURDB\" warehouse: \"COMPUTE_WH\" account: \"YOUR_ACCOUNT.us-east-2.aws\" sourceConfig: config: type: DatabaseUsage processor: type: query-parser config: filter: '' workflowConfig: openMetadataServerConfig: hostPort: http://localhost:8585/api authProvider: no-auth \"\"\" if __name__ == \"__main__\" : ingest_metadata ( config ) Congratulations on building your first metadata ingestion workflows with OpenMetadata and Prefect! Head over to the next section to see how you can run this flow on schedule.","title":"Metadata worfklows with Snowflake"},{"location":"all_workflows/#run-metadata-ingestion-usage-workflow-and-data-profiler-for-snowflake","text":"This demo will walk you through the process of creating ingestion, usage and profiling workflows.","title":"Run metadata ingestion, usage workflow and data profiler for Snowflake"},{"location":"all_workflows/#configure-your-profiling-spec","text":"OpenMetadata heavily relies on schemas. You need to understand which arguments are required and which are optional, and which are important for your use case. The easiest way to identify the required and optional arguments of any ingestion or profiling workflow is to import the OpenMetadataWorkflowConfig . From this pydantic model, you can drill down into other components using your favorite IDE e.g. Pycharm or VSCode. From there, you can again drill down into Source , SourceConfig , Sink , etc. from metadata.generated.schema.metadataIngestion.workflow import ( OpenMetadataWorkflowConfig , Source , SourceConfig , Sink )","title":"Configure your profiling spec"},{"location":"all_workflows/#sample-ingestion-and-profiling-for-snowflake","text":"Let's demonstrate how you can profile your data and add data quality tests for a Snowflake data warehouse using Prefect.","title":"Sample ingestion and profiling for Snowflake"},{"location":"all_workflows/#install-sqlalchemy-package-for-your-connector","text":"First, make sure that you have the prefect-openmetadata and the snowflake-sqlalchemy packages installed in your environment.","title":"Install SQLAlchemy package for your connector"},{"location":"all_workflows/#test-the-connection","text":"You can test the connection to your end destination using the following Prefect flow: from prefect_openmetadata.flows import validate_connection config = \"\"\" connection: config: type: Snowflake username: DEMO password: xxx account: xxx.us-east-2.aws database: YOUR_DB warehouse: COMPUTE_WH connectionType: Database \"\"\" if __name__ == \"__main__\" : validate_connection ( config )","title":"Test the connection"},{"location":"all_workflows/#ingest-your-metadata","text":"You can use the configuration below as a template - make sure to adjust it to match your username, password, your database name, and further details about the tables and schemas you want to include. You may optionally ingest your dbt manifest and dbt catalog to display dbt models and dbt docs along with your tables and views. from prefect_openmetadata.flows import ingest_metadata config = \"\"\" source: type: snowflake serviceName: snowflake serviceConnection: config: type: Snowflake username: \"xxx\" password: \"xxx\" database: \"YOURDB\" warehouse: \"COMPUTE_WH\" account: \"YOUR_ACCOUNT.us-east-2.aws\" sourceConfig: config: type: DatabaseMetadata schemaFilterPattern: includes: - YOUR_SCHEMA dbtConfigSource: dbtCatalogFilePath: /Users/you/catalog.json dbtManifestFilePath: /Users/you/manifest.json sink: type: metadata-rest config: {} workflowConfig: loggerLevel: DEBUG openMetadataServerConfig: hostPort: http://localhost:8585/api authProvider: no-auth \"\"\" if __name__ == \"__main__\" : ingest_metadata ( config )","title":"Ingest your metadata"},{"location":"all_workflows/#profile-your-data-and-add-data-quality-tests","text":"Here is a flow to profile the previously ingested data: from prefect_openmetadata.flows import profile_metadata config = \"\"\" source: type: snowflake serviceName: snowflake serviceConnection: config: type: Snowflake username: \"xxx\" password: \"xxx\" database: \"YOURDB\" warehouse: \"COMPUTE_WH\" account: \"YOUR_ACCOUNT.us-east-2.aws\" sourceConfig: config: type: Profiler fqnFilterPattern: includes: - snowflake.ANNA.ANNA_DEMO.* generateSampleData: true processor: type: orm-profiler config: test_suite: name: demo_data tests: - table: snowflake.ANNA.ANNA_DEMO.RAW_CUSTOMERS table_tests: - testCase: config: value: 100 tableTestType: tableRowCountToEqual column_tests: - columnName: ID testCase: config: minValue: 1 maxValue: 100 columnTestType: columnValuesToBeBetween sink: type: metadata-rest config: {} workflowConfig: openMetadataServerConfig: hostPort: http://localhost:8585/api authProvider: no-auth \"\"\" if __name__ == \"__main__\" : profile_metadata ( config )","title":"Profile your data and add data quality tests"},{"location":"all_workflows/#add-usage-query-patterns","text":"Finally, to ensure that you can see sample data and queries performed over given assets, run the following usage workflow ( as always, adjust to your needs ): from prefect_openmetadata.flows import ingest_metadata config = \"\"\" source: type: type: snowflake-usage serviceName: snowflake serviceConnection: config: type: Snowflake username: \"xxx\" password: \"xxx\" database: \"YOURDB\" warehouse: \"COMPUTE_WH\" account: \"YOUR_ACCOUNT.us-east-2.aws\" sourceConfig: config: type: DatabaseUsage processor: type: query-parser config: filter: '' workflowConfig: openMetadataServerConfig: hostPort: http://localhost:8585/api authProvider: no-auth \"\"\" if __name__ == \"__main__\" : ingest_metadata ( config ) Congratulations on building your first metadata ingestion workflows with OpenMetadata and Prefect! Head over to the next section to see how you can run this flow on schedule.","title":"Add usage query patterns"},{"location":"flows/","text":"prefect_openmetadata.flows Prefect flows and tasks for OpenMetadata workflows include: metadata ingestion workflow: information about your tables, view, schemas, etc usage ingestion workflow: query and audit logs showing usage patterns profiler workflow: profile data and run data validation tests Follow the main documentation for guidance on: installing and configuring Prefect and OpenMetadata , running metadata ingestion flows locally and on schedule. OpenMetadataFailedConnection Exception for failed connection attempts Source code in prefect_openmetadata/flows.py 29 30 class OpenMetadataFailedConnection ( Exception ): \"\"\"Exception for failed connection attempts\"\"\" ingest_metadata Ingests raw metadata about tables, dashboards, users, topics, pipelines, etc. The same flow is used for usage ingestion. Parameters: Name Type Description Default config str configuration spec, by default in YAML, optionally in JSON required is_json bool flag whether config is a JSON spec rather than YAML False Examples: Flow ingesting metadata using Prefect: from prefect_openmetadata.flows import ingest_metadata config = \"\"\"See an example in the section: Run ingestion flow\"\"\" if __name__ == \"__main__\" : ingest_metadata ( config ) Source code in prefect_openmetadata/flows.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @flow def ingest_metadata ( config : str , is_json : bool = False ) -> None : \"\"\" Ingests raw metadata about tables, dashboards, users, topics, pipelines, etc. The same flow is used for usage ingestion. Args: config: configuration spec, by default in YAML, optionally in JSON is_json: flag whether `config` is a JSON spec rather than YAML Examples: Flow ingesting metadata using Prefect: ```python from prefect_openmetadata.flows import ingest_metadata config = \\\"\"\" See an example in the section : Run ingestion flow \\ \"\"\" if __name__ == \"__main__\": ingest_metadata(config) ``` \"\"\" om_workflow_config = json . loads ( config ) if is_json else yaml . safe_load ( config ) om_workflow_model = OpenMetadataWorkflowConfig . parse_obj ( om_workflow_config ) run_ingestion_task ( om_workflow_model ) make_test_connection async Task maging a test connection to the specified OpenMetadata connection Parameters: Name Type Description Default conn_config TestServiceConnectionRequest connection spec as a pydantic model required Source code in prefect_openmetadata/flows.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 @task async def make_test_connection ( conn_config : TestServiceConnectionRequest ) -> None : \"\"\" Task maging a test connection to the specified OpenMetadata connection Args: conn_config: connection spec as a pydantic model \"\"\" logger = get_run_logger () connection = get_connection ( conn_config . connection . config ) try : test_connection ( connection ) logger . info ( \"Connection successful!\" ) except OpenMetadataFailedConnection as exc : logger . error ( \"Test connection failed\" ) raise exc profile_metadata Profiles metadata about tables, dashboards, users, topics, pipelines, etc. Parameters: Name Type Description Default config str configuration spec, by default in YAML, optionally in JSON required is_json bool flag whether config is a JSON spec rather than YAML False Examples: Flow profiling metadata using Prefect: from prefect_openmetadata.flows import profile_metadata config = \"\"\"See an example in the section: Run profiling flow\"\"\" if __name__ == \"__main__\" : profile_metadata ( config ) Source code in prefect_openmetadata/flows.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @flow def profile_metadata ( config : str , is_json : bool = False ) -> None : \"\"\" Profiles metadata about tables, dashboards, users, topics, pipelines, etc. Args: config: configuration spec, by default in YAML, optionally in JSON is_json: flag whether `config` is a JSON spec rather than YAML Examples: Flow profiling metadata using Prefect: ```python from prefect_openmetadata.flows import profile_metadata config = \\\"\"\" See an example in the section : Run profiling flow \\ \"\"\" if __name__ == \"__main__\": profile_metadata(config) ``` \"\"\" om_workflow_config = json . loads ( config ) if is_json else yaml . safe_load ( config ) om_workflow_model = OpenMetadataWorkflowConfig . parse_obj ( om_workflow_config ) run_profiling_task ( om_workflow_model ) run_ingestion_task async Task ingesting metadata into OpenMetadata backend Parameters: Name Type Description Default om_workflow_model OpenMetadataWorkflowConfig ingestion spec as a pydantic model required Source code in prefect_openmetadata/flows.py 33 34 35 36 37 38 39 40 41 42 43 44 45 @task async def run_ingestion_task ( om_workflow_model : OpenMetadataWorkflowConfig ) -> None : \"\"\" Task ingesting metadata into OpenMetadata backend Args: om_workflow_model: ingestion spec as a pydantic model \"\"\" workflow = PrefectOpenMetadataIngestion ( om_workflow_model ) workflow . execute () workflow . log_flow_status () workflow . raise_from_status () workflow . stop () run_profiling_task async Task profiling a given OpenMetadata source Parameters: Name Type Description Default om_workflow_model OpenMetadataWorkflowConfig profiling spec as a pydantic model required Source code in prefect_openmetadata/flows.py 74 75 76 77 78 79 80 81 82 83 84 85 86 @task async def run_profiling_task ( om_workflow_model : OpenMetadataWorkflowConfig ) -> None : \"\"\" Task profiling a given OpenMetadata source Args: om_workflow_model: profiling spec as a pydantic model \"\"\" workflow = PrefectOpenMetadataProfiler ( om_workflow_model ) workflow . execute () workflow . log_flow_status () workflow . raise_from_status () workflow . stop () validate_connection Makes a SQLAlchemy connection based on a given JSON or YAML config for testing. Go to the OpenMetadata schema definitions , to inspect required fields to connect with your desired system and crawl its metadata. Requires installing the required sqlalchemy subpackage for the relevant connector e.g. Snowflake connection requires pip install snowflake-sqlalchemy Parameters: Name Type Description Default conn_config str connection spec as a string from a JSON spec required is_json bool flag whether conn_config is a JSON spec rather than YAML False Examples: Flow testing connection using Prefect: from prefect_openmetadata.flows import validate_connection config = \"\"\" connection: config: type: Snowflake username: DEMO password: xxx account: xxx.us-east-2.aws database: YOUR_DB warehouse: COMPUTE_WH connectionType: Database \"\"\" if __name__ == \"__main__\" : validate_connection ( config ) Source code in prefect_openmetadata/flows.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 @flow def validate_connection ( conn_config : str , is_json : bool = False ): \"\"\" Makes a SQLAlchemy connection based on a given JSON or YAML config for testing. Go to the [OpenMetadata schema definitions](https://github.com/open-metadata/OpenMetadata/tree/main/catalog-rest-service/src/main/resources/json/schema/entity/services/connections), to inspect required fields to connect with your desired system and crawl its metadata. Requires installing the required sqlalchemy subpackage for the relevant connector e.g. Snowflake connection requires `pip install snowflake-sqlalchemy` Args: conn_config: connection spec as a string from a JSON spec is_json: flag whether `conn_config` is a JSON spec rather than YAML Examples: Flow testing connection using Prefect: ```python from prefect_openmetadata.flows import validate_connection config = \\\"\"\" connection : config : type : Snowflake username : DEMO password : xxx account : xxx . us - east - 2. aws database : YOUR_DB warehouse : COMPUTE_WH connectionType : Database \\ \"\"\" if __name__ == \"__main__\": validate_connection(config) ``` \"\"\" conn_spec_dict = json . loads ( conn_config ) if is_json else yaml . safe_load ( conn_config ) conn_model = TestServiceConnectionRequest . parse_obj ( conn_spec_dict ) make_test_connection ( conn_model )","title":"Flows & tasks"},{"location":"flows/#prefect_openmetadata.flows","text":"","title":"flows"},{"location":"flows/#prefect_openmetadata.flows--prefect-flows-and-tasks-for-openmetadata-workflows-include","text":"metadata ingestion workflow: information about your tables, view, schemas, etc usage ingestion workflow: query and audit logs showing usage patterns profiler workflow: profile data and run data validation tests Follow the main documentation for guidance on: installing and configuring Prefect and OpenMetadata , running metadata ingestion flows locally and on schedule.","title":"Prefect flows and tasks for OpenMetadata workflows include:"},{"location":"flows/#prefect_openmetadata.flows.OpenMetadataFailedConnection","text":"Exception for failed connection attempts Source code in prefect_openmetadata/flows.py 29 30 class OpenMetadataFailedConnection ( Exception ): \"\"\"Exception for failed connection attempts\"\"\"","title":"OpenMetadataFailedConnection"},{"location":"flows/#prefect_openmetadata.flows.ingest_metadata","text":"Ingests raw metadata about tables, dashboards, users, topics, pipelines, etc. The same flow is used for usage ingestion. Parameters: Name Type Description Default config str configuration spec, by default in YAML, optionally in JSON required is_json bool flag whether config is a JSON spec rather than YAML False Examples: Flow ingesting metadata using Prefect: from prefect_openmetadata.flows import ingest_metadata config = \"\"\"See an example in the section: Run ingestion flow\"\"\" if __name__ == \"__main__\" : ingest_metadata ( config ) Source code in prefect_openmetadata/flows.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @flow def ingest_metadata ( config : str , is_json : bool = False ) -> None : \"\"\" Ingests raw metadata about tables, dashboards, users, topics, pipelines, etc. The same flow is used for usage ingestion. Args: config: configuration spec, by default in YAML, optionally in JSON is_json: flag whether `config` is a JSON spec rather than YAML Examples: Flow ingesting metadata using Prefect: ```python from prefect_openmetadata.flows import ingest_metadata config = \\\"\"\" See an example in the section : Run ingestion flow \\ \"\"\" if __name__ == \"__main__\": ingest_metadata(config) ``` \"\"\" om_workflow_config = json . loads ( config ) if is_json else yaml . safe_load ( config ) om_workflow_model = OpenMetadataWorkflowConfig . parse_obj ( om_workflow_config ) run_ingestion_task ( om_workflow_model )","title":"ingest_metadata()"},{"location":"flows/#prefect_openmetadata.flows.make_test_connection","text":"Task maging a test connection to the specified OpenMetadata connection Parameters: Name Type Description Default conn_config TestServiceConnectionRequest connection spec as a pydantic model required Source code in prefect_openmetadata/flows.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 @task async def make_test_connection ( conn_config : TestServiceConnectionRequest ) -> None : \"\"\" Task maging a test connection to the specified OpenMetadata connection Args: conn_config: connection spec as a pydantic model \"\"\" logger = get_run_logger () connection = get_connection ( conn_config . connection . config ) try : test_connection ( connection ) logger . info ( \"Connection successful!\" ) except OpenMetadataFailedConnection as exc : logger . error ( \"Test connection failed\" ) raise exc","title":"make_test_connection()"},{"location":"flows/#prefect_openmetadata.flows.profile_metadata","text":"Profiles metadata about tables, dashboards, users, topics, pipelines, etc. Parameters: Name Type Description Default config str configuration spec, by default in YAML, optionally in JSON required is_json bool flag whether config is a JSON spec rather than YAML False Examples: Flow profiling metadata using Prefect: from prefect_openmetadata.flows import profile_metadata config = \"\"\"See an example in the section: Run profiling flow\"\"\" if __name__ == \"__main__\" : profile_metadata ( config ) Source code in prefect_openmetadata/flows.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 @flow def profile_metadata ( config : str , is_json : bool = False ) -> None : \"\"\" Profiles metadata about tables, dashboards, users, topics, pipelines, etc. Args: config: configuration spec, by default in YAML, optionally in JSON is_json: flag whether `config` is a JSON spec rather than YAML Examples: Flow profiling metadata using Prefect: ```python from prefect_openmetadata.flows import profile_metadata config = \\\"\"\" See an example in the section : Run profiling flow \\ \"\"\" if __name__ == \"__main__\": profile_metadata(config) ``` \"\"\" om_workflow_config = json . loads ( config ) if is_json else yaml . safe_load ( config ) om_workflow_model = OpenMetadataWorkflowConfig . parse_obj ( om_workflow_config ) run_profiling_task ( om_workflow_model )","title":"profile_metadata()"},{"location":"flows/#prefect_openmetadata.flows.run_ingestion_task","text":"Task ingesting metadata into OpenMetadata backend Parameters: Name Type Description Default om_workflow_model OpenMetadataWorkflowConfig ingestion spec as a pydantic model required Source code in prefect_openmetadata/flows.py 33 34 35 36 37 38 39 40 41 42 43 44 45 @task async def run_ingestion_task ( om_workflow_model : OpenMetadataWorkflowConfig ) -> None : \"\"\" Task ingesting metadata into OpenMetadata backend Args: om_workflow_model: ingestion spec as a pydantic model \"\"\" workflow = PrefectOpenMetadataIngestion ( om_workflow_model ) workflow . execute () workflow . log_flow_status () workflow . raise_from_status () workflow . stop ()","title":"run_ingestion_task()"},{"location":"flows/#prefect_openmetadata.flows.run_profiling_task","text":"Task profiling a given OpenMetadata source Parameters: Name Type Description Default om_workflow_model OpenMetadataWorkflowConfig profiling spec as a pydantic model required Source code in prefect_openmetadata/flows.py 74 75 76 77 78 79 80 81 82 83 84 85 86 @task async def run_profiling_task ( om_workflow_model : OpenMetadataWorkflowConfig ) -> None : \"\"\" Task profiling a given OpenMetadata source Args: om_workflow_model: profiling spec as a pydantic model \"\"\" workflow = PrefectOpenMetadataProfiler ( om_workflow_model ) workflow . execute () workflow . log_flow_status () workflow . raise_from_status () workflow . stop ()","title":"run_profiling_task()"},{"location":"flows/#prefect_openmetadata.flows.validate_connection","text":"Makes a SQLAlchemy connection based on a given JSON or YAML config for testing. Go to the OpenMetadata schema definitions , to inspect required fields to connect with your desired system and crawl its metadata. Requires installing the required sqlalchemy subpackage for the relevant connector e.g. Snowflake connection requires pip install snowflake-sqlalchemy Parameters: Name Type Description Default conn_config str connection spec as a string from a JSON spec required is_json bool flag whether conn_config is a JSON spec rather than YAML False Examples: Flow testing connection using Prefect: from prefect_openmetadata.flows import validate_connection config = \"\"\" connection: config: type: Snowflake username: DEMO password: xxx account: xxx.us-east-2.aws database: YOUR_DB warehouse: COMPUTE_WH connectionType: Database \"\"\" if __name__ == \"__main__\" : validate_connection ( config ) Source code in prefect_openmetadata/flows.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 @flow def validate_connection ( conn_config : str , is_json : bool = False ): \"\"\" Makes a SQLAlchemy connection based on a given JSON or YAML config for testing. Go to the [OpenMetadata schema definitions](https://github.com/open-metadata/OpenMetadata/tree/main/catalog-rest-service/src/main/resources/json/schema/entity/services/connections), to inspect required fields to connect with your desired system and crawl its metadata. Requires installing the required sqlalchemy subpackage for the relevant connector e.g. Snowflake connection requires `pip install snowflake-sqlalchemy` Args: conn_config: connection spec as a string from a JSON spec is_json: flag whether `conn_config` is a JSON spec rather than YAML Examples: Flow testing connection using Prefect: ```python from prefect_openmetadata.flows import validate_connection config = \\\"\"\" connection : config : type : Snowflake username : DEMO password : xxx account : xxx . us - east - 2. aws database : YOUR_DB warehouse : COMPUTE_WH connectionType : Database \\ \"\"\" if __name__ == \"__main__\": validate_connection(config) ``` \"\"\" conn_spec_dict = json . loads ( conn_config ) if is_json else yaml . safe_load ( conn_config ) conn_model = TestServiceConnectionRequest . parse_obj ( conn_spec_dict ) make_test_connection ( conn_model )","title":"validate_connection()"},{"location":"ingestion_workflow/","text":"prefect_openmetadata.ingestion_workflow Extension to the OpenMetadata Workflow class PrefectOpenMetadataIngestion OpenMetadata ingestion workflow that adds a method allowing to log the workflow status to the Prefect backend. Parameters: Name Type Description Default config string with a YAML or JSON configuration file required Source code in prefect_openmetadata/ingestion_workflow.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class PrefectOpenMetadataIngestion ( Workflow ): \"\"\" OpenMetadata ingestion workflow that adds a method allowing to log the workflow status to the Prefect backend. Args: config: string with a YAML or JSON configuration file \"\"\" def __int__ ( self , config : OpenMetadataWorkflowConfig ): \"\"\" Args: config: string with a YAML or JSON configuration file \"\"\" super () . __init__ ( config = config ) def log_flow_status ( self ) -> None : \"\"\" Log workflow status to the Prefect API backend \"\"\" logger = get_run_logger () logger . info ( \"Source Status: %s \" , self . source . get_status () . as_string ()) if hasattr ( self , \"stage\" ): logger . info ( \"Stage Status: %s \" , self . stage . get_status () . as_string ()) if hasattr ( self , \"sink\" ): logger . info ( \"Sink Status: %s \" , self . sink . get_status () . as_string ()) if hasattr ( self , \"bulk_sink\" ): logger . info ( \"Bulk Sink Status: %s \" , self . bulk_sink . get_status () . as_string ()) __int__ Parameters: Name Type Description Default config OpenMetadataWorkflowConfig string with a YAML or JSON configuration file required Source code in prefect_openmetadata/ingestion_workflow.py 20 21 22 23 24 25 def __int__ ( self , config : OpenMetadataWorkflowConfig ): \"\"\" Args: config: string with a YAML or JSON configuration file \"\"\" super () . __init__ ( config = config ) log_flow_status Log workflow status to the Prefect API backend Source code in prefect_openmetadata/ingestion_workflow.py 27 28 29 30 31 32 33 34 35 36 37 38 def log_flow_status ( self ) -> None : \"\"\" Log workflow status to the Prefect API backend \"\"\" logger = get_run_logger () logger . info ( \"Source Status: %s \" , self . source . get_status () . as_string ()) if hasattr ( self , \"stage\" ): logger . info ( \"Stage Status: %s \" , self . stage . get_status () . as_string ()) if hasattr ( self , \"sink\" ): logger . info ( \"Sink Status: %s \" , self . sink . get_status () . as_string ()) if hasattr ( self , \"bulk_sink\" ): logger . info ( \"Bulk Sink Status: %s \" , self . bulk_sink . get_status () . as_string ())","title":"Ingestion workflow module"},{"location":"ingestion_workflow/#prefect_openmetadata.ingestion_workflow","text":"Extension to the OpenMetadata Workflow class","title":"ingestion_workflow"},{"location":"ingestion_workflow/#prefect_openmetadata.ingestion_workflow.PrefectOpenMetadataIngestion","text":"OpenMetadata ingestion workflow that adds a method allowing to log the workflow status to the Prefect backend. Parameters: Name Type Description Default config string with a YAML or JSON configuration file required Source code in prefect_openmetadata/ingestion_workflow.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class PrefectOpenMetadataIngestion ( Workflow ): \"\"\" OpenMetadata ingestion workflow that adds a method allowing to log the workflow status to the Prefect backend. Args: config: string with a YAML or JSON configuration file \"\"\" def __int__ ( self , config : OpenMetadataWorkflowConfig ): \"\"\" Args: config: string with a YAML or JSON configuration file \"\"\" super () . __init__ ( config = config ) def log_flow_status ( self ) -> None : \"\"\" Log workflow status to the Prefect API backend \"\"\" logger = get_run_logger () logger . info ( \"Source Status: %s \" , self . source . get_status () . as_string ()) if hasattr ( self , \"stage\" ): logger . info ( \"Stage Status: %s \" , self . stage . get_status () . as_string ()) if hasattr ( self , \"sink\" ): logger . info ( \"Sink Status: %s \" , self . sink . get_status () . as_string ()) if hasattr ( self , \"bulk_sink\" ): logger . info ( \"Bulk Sink Status: %s \" , self . bulk_sink . get_status () . as_string ())","title":"PrefectOpenMetadataIngestion"},{"location":"ingestion_workflow/#prefect_openmetadata.ingestion_workflow.PrefectOpenMetadataIngestion.__int__","text":"Parameters: Name Type Description Default config OpenMetadataWorkflowConfig string with a YAML or JSON configuration file required Source code in prefect_openmetadata/ingestion_workflow.py 20 21 22 23 24 25 def __int__ ( self , config : OpenMetadataWorkflowConfig ): \"\"\" Args: config: string with a YAML or JSON configuration file \"\"\" super () . __init__ ( config = config )","title":"__int__()"},{"location":"ingestion_workflow/#prefect_openmetadata.ingestion_workflow.PrefectOpenMetadataIngestion.log_flow_status","text":"Log workflow status to the Prefect API backend Source code in prefect_openmetadata/ingestion_workflow.py 27 28 29 30 31 32 33 34 35 36 37 38 def log_flow_status ( self ) -> None : \"\"\" Log workflow status to the Prefect API backend \"\"\" logger = get_run_logger () logger . info ( \"Source Status: %s \" , self . source . get_status () . as_string ()) if hasattr ( self , \"stage\" ): logger . info ( \"Stage Status: %s \" , self . stage . get_status () . as_string ()) if hasattr ( self , \"sink\" ): logger . info ( \"Sink Status: %s \" , self . sink . get_status () . as_string ()) if hasattr ( self , \"bulk_sink\" ): logger . info ( \"Bulk Sink Status: %s \" , self . bulk_sink . get_status () . as_string ())","title":"log_flow_status()"},{"location":"install_openmetadata/","text":"Install OpenMetadata Requirements Please ensure your host system meets the requirements listed below: Python 3.8+ Docker 20.10.0+ Docker Compose Installing OpenMetadata Clone the prefect-openmetadata repository First, clone the latest version of the prefect-openmetadata Prefect Collection. Then, navigate to the directory openmetadata-docker containing the docker-compose.yml file with the minimal requirements to get started with OpenMetadata. Start OpenMetadata containers You can start the containers with OpenMetadata components using: docker compose up -d This will create a docker network and containers with the following services: openmetadata_mysql - metadata store that serves as a persistence layer holding your metadata, openmetadata_elasticsearch - indexing service to search the metadata catalog, openmetadata_server - the OpenMetadata UI and API server allowing you to discover insights and interact with your metadata. Wait a couple of minutes until the setup is finished. To check the status of all services, you may run the docker compose ps command to investigate the status of all Docker containers: NAME COMMAND SERVICE STATUS PORTS openmetadata_elasticsearch \"/tini -- /usr/local\u2026\" elasticsearch running 0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp openmetadata_mysql \"/entrypoint.sh mysq\u2026\" mysql running (healthy) 33060-33061/tcp openmetadata_server \"./openmetadata-star\u2026\" openmetadata-server running 0.0.0.0:8585->8585/tcp Confirm you can access the OpenMetadata UI Visit the following URL to confirm you can access the UI and start exploring OpenMetadata: http://localhost:8585 You should see a page similar to the following as the landing page for the OpenMetadata UI. Why should you use Prefect for metadata ingestion? The challenge with the metadata ingestion is to ensure that this process can be automated and can run reliably , either on a regular interval, or ad-hoc. This is where Prefect can help. Prefect 2.0 is a general-purpose workflow orchestration platform allowing you to build, run, schedule, and operationalize your data pipelines at scale. It supports both batch and streaming workflows and provides an excellent developer experience allowing you to run your flows locally and seamlessly move to production and to Cloud when you\u2019re ready. Among many other features , it natively supports: dynamic runtime-discoverable and modular workflows, passing data between tasks, running your workflows on various execution platforms (on-prem, cloud, Docker, Kubernetes) while maintaining privacy via a hybrid execution model , scaling out for parallel and concurrent execution with async, Dask, and Ray , various integrations through Prefect Collections - such as this one! Install Prefect You can install Prefect using a single command: pip install -U \"prefect>=2.0b\" To make sure that OpenMetadata logs are stored in the Prefect backend and displayed in the UI, run the following command: prefect config set PREFECT_LOGGING_EXTRA_LOGGERS = 'Ingestion,OMetaAPI,Metadata,Profiler,Utils' This way, Prefect is aware of the extra loggers from OpenMetadata . When you install Prefect 2.0, this will not only install the client library, but also an embedded API server and UI, which can optionally be started using: prefect orion start If you navigate to the URL, you\u2019ll be able to access a locally running Prefect Orion UI: http://localhost:4200","title":"Install OpenMetadata"},{"location":"install_openmetadata/#install-openmetadata","text":"","title":"Install OpenMetadata"},{"location":"install_openmetadata/#requirements","text":"Please ensure your host system meets the requirements listed below: Python 3.8+ Docker 20.10.0+ Docker Compose","title":"Requirements"},{"location":"install_openmetadata/#installing-openmetadata","text":"","title":"Installing OpenMetadata"},{"location":"install_openmetadata/#clone-the-prefect-openmetadata-repository","text":"First, clone the latest version of the prefect-openmetadata Prefect Collection. Then, navigate to the directory openmetadata-docker containing the docker-compose.yml file with the minimal requirements to get started with OpenMetadata.","title":"Clone the prefect-openmetadata repository"},{"location":"install_openmetadata/#start-openmetadata-containers","text":"You can start the containers with OpenMetadata components using: docker compose up -d This will create a docker network and containers with the following services: openmetadata_mysql - metadata store that serves as a persistence layer holding your metadata, openmetadata_elasticsearch - indexing service to search the metadata catalog, openmetadata_server - the OpenMetadata UI and API server allowing you to discover insights and interact with your metadata. Wait a couple of minutes until the setup is finished. To check the status of all services, you may run the docker compose ps command to investigate the status of all Docker containers: NAME COMMAND SERVICE STATUS PORTS openmetadata_elasticsearch \"/tini -- /usr/local\u2026\" elasticsearch running 0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp openmetadata_mysql \"/entrypoint.sh mysq\u2026\" mysql running (healthy) 33060-33061/tcp openmetadata_server \"./openmetadata-star\u2026\" openmetadata-server running 0.0.0.0:8585->8585/tcp","title":"Start OpenMetadata containers"},{"location":"install_openmetadata/#confirm-you-can-access-the-openmetadata-ui","text":"Visit the following URL to confirm you can access the UI and start exploring OpenMetadata: http://localhost:8585 You should see a page similar to the following as the landing page for the OpenMetadata UI.","title":"Confirm you can access the OpenMetadata UI"},{"location":"install_openmetadata/#why-should-you-use-prefect-for-metadata-ingestion","text":"The challenge with the metadata ingestion is to ensure that this process can be automated and can run reliably , either on a regular interval, or ad-hoc. This is where Prefect can help. Prefect 2.0 is a general-purpose workflow orchestration platform allowing you to build, run, schedule, and operationalize your data pipelines at scale. It supports both batch and streaming workflows and provides an excellent developer experience allowing you to run your flows locally and seamlessly move to production and to Cloud when you\u2019re ready. Among many other features , it natively supports: dynamic runtime-discoverable and modular workflows, passing data between tasks, running your workflows on various execution platforms (on-prem, cloud, Docker, Kubernetes) while maintaining privacy via a hybrid execution model , scaling out for parallel and concurrent execution with async, Dask, and Ray , various integrations through Prefect Collections - such as this one!","title":"Why should you use Prefect for metadata ingestion?"},{"location":"install_openmetadata/#install-prefect","text":"You can install Prefect using a single command: pip install -U \"prefect>=2.0b\" To make sure that OpenMetadata logs are stored in the Prefect backend and displayed in the UI, run the following command: prefect config set PREFECT_LOGGING_EXTRA_LOGGERS = 'Ingestion,OMetaAPI,Metadata,Profiler,Utils' This way, Prefect is aware of the extra loggers from OpenMetadata . When you install Prefect 2.0, this will not only install the client library, but also an embedded API server and UI, which can optionally be started using: prefect orion start If you navigate to the URL, you\u2019ll be able to access a locally running Prefect Orion UI: http://localhost:4200","title":"Install Prefect"},{"location":"profiler_workflow/","text":"prefect_openmetadata.profiler_workflow Extension to the OpenMetadata ProfilerWorkflow class PrefectOpenMetadataProfiler OpenMetadata profiler workflow that adds a method allowing to log the workflow status to the Prefect backend. Parameters: Name Type Description Default config string with a YAML or JSON configuration file required Source code in prefect_openmetadata/profiler_workflow.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class PrefectOpenMetadataProfiler ( ProfilerWorkflow ): \"\"\" OpenMetadata profiler workflow that adds a method allowing to log the workflow status to the Prefect backend. Args: config: string with a YAML or JSON configuration file \"\"\" def __int__ ( self , config : OpenMetadataWorkflowConfig ): \"\"\" Args: config: string with a YAML or JSON configuration file \"\"\" super () . __init__ ( config = config ) def log_flow_status ( self ) -> None : \"\"\" Log workflow status to the Prefect API backend \"\"\" logger = get_run_logger () logger . info ( \"Source Status: %s \" , self . source_status . as_string ()) logger . info ( \"Processir Status: %s \" , self . processor . get_status () . as_string ()) if hasattr ( self , \"sink\" ): logger . info ( \"Sink Status: %s \" , self . sink . get_status () . as_string ()) __int__ Parameters: Name Type Description Default config OpenMetadataWorkflowConfig string with a YAML or JSON configuration file required Source code in prefect_openmetadata/profiler_workflow.py 20 21 22 23 24 25 def __int__ ( self , config : OpenMetadataWorkflowConfig ): \"\"\" Args: config: string with a YAML or JSON configuration file \"\"\" super () . __init__ ( config = config ) log_flow_status Log workflow status to the Prefect API backend Source code in prefect_openmetadata/profiler_workflow.py 27 28 29 30 31 32 33 34 35 def log_flow_status ( self ) -> None : \"\"\" Log workflow status to the Prefect API backend \"\"\" logger = get_run_logger () logger . info ( \"Source Status: %s \" , self . source_status . as_string ()) logger . info ( \"Processir Status: %s \" , self . processor . get_status () . as_string ()) if hasattr ( self , \"sink\" ): logger . info ( \"Sink Status: %s \" , self . sink . get_status () . as_string ())","title":"Profiler workflow module"},{"location":"profiler_workflow/#prefect_openmetadata.profiler_workflow","text":"Extension to the OpenMetadata ProfilerWorkflow class","title":"profiler_workflow"},{"location":"profiler_workflow/#prefect_openmetadata.profiler_workflow.PrefectOpenMetadataProfiler","text":"OpenMetadata profiler workflow that adds a method allowing to log the workflow status to the Prefect backend. Parameters: Name Type Description Default config string with a YAML or JSON configuration file required Source code in prefect_openmetadata/profiler_workflow.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class PrefectOpenMetadataProfiler ( ProfilerWorkflow ): \"\"\" OpenMetadata profiler workflow that adds a method allowing to log the workflow status to the Prefect backend. Args: config: string with a YAML or JSON configuration file \"\"\" def __int__ ( self , config : OpenMetadataWorkflowConfig ): \"\"\" Args: config: string with a YAML or JSON configuration file \"\"\" super () . __init__ ( config = config ) def log_flow_status ( self ) -> None : \"\"\" Log workflow status to the Prefect API backend \"\"\" logger = get_run_logger () logger . info ( \"Source Status: %s \" , self . source_status . as_string ()) logger . info ( \"Processir Status: %s \" , self . processor . get_status () . as_string ()) if hasattr ( self , \"sink\" ): logger . info ( \"Sink Status: %s \" , self . sink . get_status () . as_string ())","title":"PrefectOpenMetadataProfiler"},{"location":"profiler_workflow/#prefect_openmetadata.profiler_workflow.PrefectOpenMetadataProfiler.__int__","text":"Parameters: Name Type Description Default config OpenMetadataWorkflowConfig string with a YAML or JSON configuration file required Source code in prefect_openmetadata/profiler_workflow.py 20 21 22 23 24 25 def __int__ ( self , config : OpenMetadataWorkflowConfig ): \"\"\" Args: config: string with a YAML or JSON configuration file \"\"\" super () . __init__ ( config = config )","title":"__int__()"},{"location":"profiler_workflow/#prefect_openmetadata.profiler_workflow.PrefectOpenMetadataProfiler.log_flow_status","text":"Log workflow status to the Prefect API backend Source code in prefect_openmetadata/profiler_workflow.py 27 28 29 30 31 32 33 34 35 def log_flow_status ( self ) -> None : \"\"\" Log workflow status to the Prefect API backend \"\"\" logger = get_run_logger () logger . info ( \"Source Status: %s \" , self . source_status . as_string ()) logger . info ( \"Processir Status: %s \" , self . processor . get_status () . as_string ()) if hasattr ( self , \"sink\" ): logger . info ( \"Sink Status: %s \" , self . sink . get_status () . as_string ())","title":"log_flow_status()"},{"location":"run_ingestion_flow/","text":"Run metadata ingestion Configure your ingestion spec In the Install OpenMetadata section, you cloned the prefect-openmetadata repository. This repository contains a directory example-data which you can use to ingest sample data into your OpenMetadata backend using Prefect. Here is a configuration spec you can use in your flow to ingest that sample data: source : type : sample-data serviceName : sample_data serviceConnection : config : type : SampleData sampleDataFolder : \"example-data\" sourceConfig : {} sink : type : metadata-rest config : {} workflowConfig : openMetadataServerConfig : hostPort : http://localhost:8585/api authProvider : no-auth Run ingestion workflow locally Now you can paste the config from above as a string into your flow and run it: from prefect_openmetadata.flows import ingest_metadata config = \"\"\" source: type: sample-data serviceName: sample_data serviceConnection: config: type: SampleData sampleDataFolder: \"example-data\" sourceConfig: {} sink: type: metadata-rest config: {} workflowConfig: openMetadataServerConfig: hostPort: http://localhost:8585/api authProvider: no-auth \"\"\" if __name__ == \"__main__\" : ingest_metadata ( config ) After running your flow, you should see new users , datasets , dashboards, and other metadata in your OpenMetadata UI. Also, your Prefect UI will display the workflow run and will show the logs with details on which source system has been scanned and which data has been ingested. Congratulations on building your first metadata ingestion workflow with OpenMetadata and Prefect! Head over to the next section to see how you can run this flow on schedule.","title":"Run ingestion flow"},{"location":"run_ingestion_flow/#run-metadata-ingestion","text":"","title":"Run metadata ingestion"},{"location":"run_ingestion_flow/#configure-your-ingestion-spec","text":"In the Install OpenMetadata section, you cloned the prefect-openmetadata repository. This repository contains a directory example-data which you can use to ingest sample data into your OpenMetadata backend using Prefect. Here is a configuration spec you can use in your flow to ingest that sample data: source : type : sample-data serviceName : sample_data serviceConnection : config : type : SampleData sampleDataFolder : \"example-data\" sourceConfig : {} sink : type : metadata-rest config : {} workflowConfig : openMetadataServerConfig : hostPort : http://localhost:8585/api authProvider : no-auth","title":"Configure your ingestion spec"},{"location":"run_ingestion_flow/#run-ingestion-workflow-locally","text":"Now you can paste the config from above as a string into your flow and run it: from prefect_openmetadata.flows import ingest_metadata config = \"\"\" source: type: sample-data serviceName: sample_data serviceConnection: config: type: SampleData sampleDataFolder: \"example-data\" sourceConfig: {} sink: type: metadata-rest config: {} workflowConfig: openMetadataServerConfig: hostPort: http://localhost:8585/api authProvider: no-auth \"\"\" if __name__ == \"__main__\" : ingest_metadata ( config ) After running your flow, you should see new users , datasets , dashboards, and other metadata in your OpenMetadata UI. Also, your Prefect UI will display the workflow run and will show the logs with details on which source system has been scanned and which data has been ingested. Congratulations on building your first metadata ingestion workflow with OpenMetadata and Prefect! Head over to the next section to see how you can run this flow on schedule.","title":"Run ingestion workflow locally"},{"location":"schedule_ingestion_flow/","text":"Schedule and deploy metadata ingestion flows Schedule your OpenMetadata ingestion flows with Prefect Ingesting your data via manually executed scripts is great for initial exploration, but in order to build a reliable metadata platform, you need to run those workflows on a regular cadence. That\u2019s where you can leverage Prefect schedules and deployments . Here is how you can add a DeploymentSpec to your flow to ensure that your metadata gets refreshed every 15 minutes: # ingestion_flow.py from datetime import timedelta from prefect.deployments import DeploymentSpec from prefect.flow_runners import SubprocessFlowRunner from prefect.orion.schemas.schedules import IntervalSchedule from prefect_openmetadata.flows import ingest_metadata config = \"\"\"See previous examples\"\"\" DeploymentSpec ( name = \"openmetadata-dev\" , flow = ingest_metadata , parameters = dict ( config = config ), flow_runner = SubprocessFlowRunner (), schedule = IntervalSchedule ( interval = timedelta ( minutes = 15 )), ) Here is an explanation of the DeploymentSpec arguments: name - specifies the name of the deployment - you could use it to differentiate between a deployment for development and production environment flow - points to the flow object, i.e. the flow function name flow_runner - specifies how the flow run should be deployed; this allows you to deploy the flow run as a docker container, a Kubernetes job, or as a local subprocess - for example, you can deploy it as a subprocess running in a Conda virtual environment named \"openmetadata\" using the syntax: SubprocessFlowRunner ( condaenv = \"openmetadata\" ) schedule - allows you to choose and customize your desired schedule class; in this example, we are using a simple IntervalSchedule triggering a new flow run every 15 minutes. With the asynchronous scheduling service in Prefect 2.0, you could even schedule your flow to run every 10 seconds if you need your metadata to be always up-to-date. To deploy this scheduled workflow to Prefect, run the following command from your CLI: prefect deployment create ingestion_flow . py Deploy your execution layer to run your flows So far, we\u2019ve looked at how you can create and schedule your workflow, but where does this code actually run? This is a place where the concepts of storage , work queues, and agents become important. But don\u2019t worry - all you need to know to get started is running one CLI command for each of those concepts. 1) Storage Storage is used to tell Prefect where your workflow code lives. To configure storage, run: prefect storage create The CLI will guide you through the process to select the storage of your choice - to get started you can select the Local Storage and choose some path in your file system. You can then directly select it as your default storage. 2) Work Queue Work queues collect scheduled runs and agents pick those up from the queue. To create a default work queue, run: prefect work - queue create default 3) Agent Agents are lightweight processes that poll their work queues for scheduled runs and execute workflows on the infrastructure you specified on the DeploymentSpec \u2019s flow_runner . To create an agent corresponding to the default work queue, run: prefect agent start default That\u2019s all you need! Once you have executed those three commands, your scheduled deployments ( such as the one we defined using ingestion_flow.py above ) are now scheduled, and Prefect will ensure that your metadata stays up-to-date. You can observe the state of your metadata ingestion workflows from the Prefect Orion UI . The UI will also include detailed logs showing which metadata got updated to ensure your data platform remains healthy and observable. Using Prefect 2.0 in the Cloud If you want to move beyond this local installation, you can deploy Prefect 2.0 to run your OpenMetadata ingestion workflows by: self-hosting the orchestration layer - see the list of resources on Prefect Discourse , or signing up for Prefect Cloud 2.0 - the following page will walk you through the process. For various deployment options of OpenMetadata, check the \u201cDeploy\u201d section of this documentation . Questions about using OpenMetadata with Prefect If you have any questions about configuring Prefect, post your question on Prefect Discourse or in the Prefect Community Slack . And if you need support for OpenMetadata, get in touch on OpenMetadata Slack .","title":"Schedule ingestion flow"},{"location":"schedule_ingestion_flow/#schedule-and-deploy-metadata-ingestion-flows","text":"","title":"Schedule and deploy metadata ingestion flows"},{"location":"schedule_ingestion_flow/#schedule-your-openmetadata-ingestion-flows-with-prefect","text":"Ingesting your data via manually executed scripts is great for initial exploration, but in order to build a reliable metadata platform, you need to run those workflows on a regular cadence. That\u2019s where you can leverage Prefect schedules and deployments . Here is how you can add a DeploymentSpec to your flow to ensure that your metadata gets refreshed every 15 minutes: # ingestion_flow.py from datetime import timedelta from prefect.deployments import DeploymentSpec from prefect.flow_runners import SubprocessFlowRunner from prefect.orion.schemas.schedules import IntervalSchedule from prefect_openmetadata.flows import ingest_metadata config = \"\"\"See previous examples\"\"\" DeploymentSpec ( name = \"openmetadata-dev\" , flow = ingest_metadata , parameters = dict ( config = config ), flow_runner = SubprocessFlowRunner (), schedule = IntervalSchedule ( interval = timedelta ( minutes = 15 )), ) Here is an explanation of the DeploymentSpec arguments: name - specifies the name of the deployment - you could use it to differentiate between a deployment for development and production environment flow - points to the flow object, i.e. the flow function name flow_runner - specifies how the flow run should be deployed; this allows you to deploy the flow run as a docker container, a Kubernetes job, or as a local subprocess - for example, you can deploy it as a subprocess running in a Conda virtual environment named \"openmetadata\" using the syntax: SubprocessFlowRunner ( condaenv = \"openmetadata\" ) schedule - allows you to choose and customize your desired schedule class; in this example, we are using a simple IntervalSchedule triggering a new flow run every 15 minutes. With the asynchronous scheduling service in Prefect 2.0, you could even schedule your flow to run every 10 seconds if you need your metadata to be always up-to-date. To deploy this scheduled workflow to Prefect, run the following command from your CLI: prefect deployment create ingestion_flow . py","title":"Schedule your OpenMetadata ingestion flows with Prefect"},{"location":"schedule_ingestion_flow/#deploy-your-execution-layer-to-run-your-flows","text":"So far, we\u2019ve looked at how you can create and schedule your workflow, but where does this code actually run? This is a place where the concepts of storage , work queues, and agents become important. But don\u2019t worry - all you need to know to get started is running one CLI command for each of those concepts. 1) Storage Storage is used to tell Prefect where your workflow code lives. To configure storage, run: prefect storage create The CLI will guide you through the process to select the storage of your choice - to get started you can select the Local Storage and choose some path in your file system. You can then directly select it as your default storage. 2) Work Queue Work queues collect scheduled runs and agents pick those up from the queue. To create a default work queue, run: prefect work - queue create default 3) Agent Agents are lightweight processes that poll their work queues for scheduled runs and execute workflows on the infrastructure you specified on the DeploymentSpec \u2019s flow_runner . To create an agent corresponding to the default work queue, run: prefect agent start default That\u2019s all you need! Once you have executed those three commands, your scheduled deployments ( such as the one we defined using ingestion_flow.py above ) are now scheduled, and Prefect will ensure that your metadata stays up-to-date. You can observe the state of your metadata ingestion workflows from the Prefect Orion UI . The UI will also include detailed logs showing which metadata got updated to ensure your data platform remains healthy and observable.","title":"Deploy your execution layer to run your flows"},{"location":"schedule_ingestion_flow/#using-prefect-20-in-the-cloud","text":"If you want to move beyond this local installation, you can deploy Prefect 2.0 to run your OpenMetadata ingestion workflows by: self-hosting the orchestration layer - see the list of resources on Prefect Discourse , or signing up for Prefect Cloud 2.0 - the following page will walk you through the process. For various deployment options of OpenMetadata, check the \u201cDeploy\u201d section of this documentation .","title":"Using Prefect 2.0 in the Cloud"},{"location":"schedule_ingestion_flow/#questions-about-using-openmetadata-with-prefect","text":"If you have any questions about configuring Prefect, post your question on Prefect Discourse or in the Prefect Community Slack . And if you need support for OpenMetadata, get in touch on OpenMetadata Slack .","title":"Questions about using OpenMetadata with Prefect"}]}